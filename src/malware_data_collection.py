from ast import keyword
from operator import le
import os
import json, time
from sys import base_exec_prefix
from bs4 import BeautifulSoup
import requests
from collections import defaultdict

from scipy import random # generates default key if missing 

def GatherData(links):
    link = 0
    while link<len(links):
        sub_dict={}
        remain =len(links)- link+1
        print('Loading:{} & Remaining:{}'.format(link+1,remain-1))
        try:
            print('Requesting:{}'.format(links[link]))
            response = requests.get(links[link], timeout = 20)
            if response.status_code == 200:
                Soup = BeautifulSoup(response.content,'html5lib')
                header= Soup.find('h2',{'id':'references'}).find_next_sibling('div')
                refrences = []
                for r in header.find_all('ol'):
                    for l in r.find_all('li'):
                        refrences.append(l.find('a').get('href'))
                    pass
                pass
                MainTable = Soup.find('table',{'class':'techniques-used'})
                for data in MainTable.find('tbody').find_all('tr'):
                    sub_data = defaultdict(list)
                    print('Domain:{}'.format(data.find_all('td')[0].text.strip()))
                    print('Technique ID:{}'.format(data.find_all('td')[1].text.strip()))
                    if len(data.find_all('td')[2].text.strip())>5:
                        print('Technique Name: {}'.format(data.find_all('td')[2].text.strip()))
                        sub_data['tech_name'] = data.find_all('td')[2].text.strip()
                    else:
                        print('Technique Name: {}'.format(data.find_all('td')[3].text.strip()))
                        sub_data['tech_name'] = data.find_all('td')[3].text.strip()
                    pass
                    #print(data.find_all('td')[-1].find_all('a'))
                    for re in data.find_all('td')[-1].find_all('a'):
                        print('Ref Link:{} & Ref Number:{}'.format(re.get('href'),re.text))
                    pass
                    sub_data['domain'] = data.find_all('td')[0].text.strip()
                    sub_data['ID'] = data.find_all('td')[1].text.strip()
                    print(sub_data)
                    sub_dict[links] = sub_data
                pass
                try: # wait until finish end before dumping ! 
                    with open('training_malware.json','a', encoding='utf-8') as outfile:
                        json.dump(sub_dict, outfile,indent=4)
                    pass
                    print('\t\t\t  Data Save Successfully  ')
                except BaseException as E:
                    print(E)
                pass

        except BaseException as E:
            print(E)
        pass
        link+=1
        time.sleep(random.randint(5,8))
    pass

if __name__ == '__main__':
    dataset = defaultdict(list)
    # loop through enterprise attack-pattern:
    root_folder = '../data/'
    folder_names = ['ics-malware', 'malware']
    for folder_name in folder_names: 
        folder = os.path.join(root_folder, folder_name)
        for filename in os.listdir(folder): 
            if filename.endswith('.json'):
                with open(os.path.join(folder, filename)) as file:
                    file_json = json.load(file)["objects"][0]
                    # retrieve information:
                    if 'x_mitre_aliases' in file_json:
                        attack_names = file_json['x_mitre_aliases']
                    else:
                        attack_names = [file_json['name']]
                    pass
                    urls = []  
                    for ref in file_json['external_references']:
                        for key, value in ref.items():
                            if key.__contains__('url'):
                                if value.__contains__('https://attack.mitre.org/'):
                                    #print(value)
                                    urls.append(value)
                                pass
                            '''if key.__contains__('description'):
                                url = ref.get('url') # retrieve url 
                                print(url)
                                urls.append(url)'''
                        pass
                    pass
                    if len(urls) > 0:
                        GatherData(urls)
                pass
            pass
        pass
    pass
        
    """for name, dataset in [('malware_information.json', dataset)]:
        print(name, len(dataset))
        with open(name, 'w') as file:
            json.dump(dataset, file)
"""